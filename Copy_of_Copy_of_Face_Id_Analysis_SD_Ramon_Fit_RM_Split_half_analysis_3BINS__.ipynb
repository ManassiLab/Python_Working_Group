{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManassiLab/Python_Working_Group/blob/main/Copy_of_Copy_of_Face_Id_Analysis_SD_Ramon_Fit_RM_Split_half_analysis_3BINS__.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Packages\n",
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "#\n",
        "\n",
        "# In[1]:\n",
        "%pip install lmfit\n",
        "import sys\n",
        "sys.path.insert(-1, \"c:\\python\\lib\\site-packages\\lmfit\")\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from lmfit import Model\n",
        "from numpy import exp, loadtxt, sqrt, std, mean\n",
        "from scipy import stats\n",
        "from scipy.stats import sem\n",
        "import sys\n",
        "import numpy\n",
        "numpy.set_printoptions(threshold=sys.maxsize)\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "pd.set_option('display.max_columns', 30)\n",
        "pd.set_option('display.max_rows', None)\n",
        "import math\n",
        "import statistics\n",
        "from scipy.stats import pearsonr"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Uomr7wCnMcO4",
        "outputId": "4c59881f-4773-4ce4-9f0d-6efcc0429ea2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lmfit in /usr/local/lib/python3.7/dist-packages (1.0.3)\n",
            "Requirement already satisfied: uncertainties>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from lmfit) (3.1.6)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from lmfit) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.7/dist-packages (from lmfit) (1.4.1)\n",
            "Requirement already satisfied: asteval>=0.9.22 in /usr/local/lib/python3.7/dist-packages (from lmfit) (0.9.27)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from asteval>=0.9.22->lmfit) (4.11.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from uncertainties>=3.0.1->lmfit) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->asteval>=0.9.22->lmfit) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->asteval>=0.9.22->lmfit) (3.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parameters**"
      ],
      "metadata": {
        "id": "4q2eGqLJNTuZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seNqEKghFXfL",
        "outputId": "cf091515-5b93-4a42-c293-733a06322f63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#FolderPathUsers = 'Fiammetta/Desktop/tirocinio' #OR, 'mauromanassi/Dropbox/MANASSILAB/zzz_FIAMMETTA' \n",
        "print(os. getcwd())\n",
        "\n",
        "\n",
        "def RunAnalysis(path):\n",
        "\n",
        "### Parameters\n",
        "\n",
        "    #### User's Input\n",
        "    halfway = 74 # circular space correction (74 morphs, 90 orientation)\n",
        "\n",
        "\n",
        "### Looks For Files In Folder\n",
        "\n",
        "    ##### read all csv files\n",
        "    files = os.listdir(path) # list of ALL files\n",
        "    print(files)\n",
        "    files_csv = list(filter(lambda x: x[-4:] == '.csv' , files)) # list of all CSV files\n",
        "    print(files_csv)\n",
        "    FileNameList = []\n",
        "    for file in files_csv:\n",
        "        FileName = file[-7:-4]\n",
        "        FileNameList.append(FileName) #to give random name to file saved NOT USED\n",
        "\n",
        "    ##### if result exists, don't run function again\n",
        "    ResultPath = path + 'results/'\n",
        "\n",
        "    ##### show CSV files to analyze\n",
        "    print('FILES FOUND AND READ:',files_csv)\n",
        "\n",
        "### Data Assembling\n",
        "\n",
        "    #MAIN VARIABLES\n",
        "    ActualStim = 'stimulusID'\n",
        "    #StartResp = 'StartBar'\n",
        "    Response = 'morphID'\n",
        "    trialsLoop = 'trialNumber' #trialnum per block trialsLoop.thisRepN\n",
        "    blocksLoop = 'blockNumber' #blocknum blocksLoop.thisRepN\n",
        "    #GaborTime = 'GaborTime'\n",
        "    #MaskTime = 'MaskTime'\n",
        "    #ISItime = 'ISI' #blocknum blocksLoop.thisRepN\n",
        "    #TimeResp = 'RespTime'\n",
        "    #ITI = 'ITI'\n",
        "    Location = 'stimLocationDeg'\n",
        "    #Gender = '*Geschlecht (M/W/anders)'\n",
        "    #age = '*Geburtsjahr '\n",
        "    #PersonalCode = 'Bitte hier eingeben: (1) die ersten 2 Buchstaben des Vornamens Ihrer Mutter, (2) der Tag (des Monats) an dem Sie geboren wurden, (3) die letzten 2 Buchstaben Ihres Vornamens, (4) der Tag (des Monats) an dem Ihre Mutter geboren wurde (Bsp: LY16KE26)'\n",
        "    blockType_ = 'blockType'\n",
        "    error_corr = 'errorCorrected'\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    # define a void list to store data\n",
        "    data_list = pd.DataFrame() #dataframe\n",
        "\n",
        "    # transformation\n",
        "    #num_filter = re.compile(r'\\d+')\n",
        "\n",
        "    #read useful columns, save in all_data\n",
        "    for file in  files_csv:\n",
        "        tmp = (pd.read_csv(path + file)[[ActualStim, Response, error_corr, trialsLoop, blocksLoop,Location,blockType_]])\n",
        "        all_data = pd.concat([data_list, tmp],ignore_index=True) #if all together: ignore_index=True\n",
        "        \n",
        "    #print(all_data)\n",
        "\n",
        "    \n",
        "    # computes trial number per block, block number and total trial number\n",
        "    file_csv = len(files_csv) #only for online studies, total of files analyzed\n",
        "    trials = (np.nanmax(all_data[trialsLoop])) #+1 because it starts at 0\n",
        "    blocks = (np.nanmax(all_data[blocksLoop])) #+1 because it starts at 0\n",
        "\n",
        "    PerFile = (trials*blocks) # Trials in each csv file\n",
        "    BlocksTotal = blocks*file_csv # Blocks in all csv file\n",
        "    TotalTrial = trials * blocks * file_csv # SuperTotal Trials\n",
        "\n",
        "    print(file_csv,'file_csv(s):\\n', int(trials),'trials/blocks.', int(blocks),'blocks - each file_csv.\\n', int(PerFile) ,'trials - each file_csv.\\n', int(BlocksTotal),'total blocks TOTAL.', int(TotalTrial),'total trials TOTAL.')\n",
        "\n",
        "    # delete invalid rows and useless columns\n",
        "    all_data.dropna(axis = 0, how = 'any', inplace = True) # exclude NaN (often between blocks)\n",
        "    all_data = all_data[all_data['blockType']=='Experiment']\n",
        "    #print(all_data)\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "### Data Assembling\n",
        "\n",
        "    #plot_dataTOTAL = all_data\n",
        "\n",
        "    xdata = all_data['stimulusID']\n",
        "    CurrStim = np.array(xdata)\n",
        "    Respdata = all_data['morphID']\n",
        "    ydata = all_data[error_corr] #np.array(Respdata)-np.array(xdata) # ERROR\n",
        "    \n",
        "            \n",
        "            \n",
        "    # Filter for outliers in error(ydata)\n",
        "   # if SDcutoffYN:\n",
        "   #     ErrorCutoff = 2 * std(ydata) # otherwise ErrorCutoff at the beginning\n",
        "   # if FilterYN == 1:\n",
        "   #     xdata = xdata[abs(ydata) < ErrorCutoff]\n",
        "   #     CurrStim = CurrStim[abs(ydata) < ErrorCutoff]\n",
        "   #     #Blocks_ = Blocks_[abs(ydata) < ErrorCutoff]\n",
        "   #     ydata = ydata[abs(ydata) < ErrorCutoff]\n",
        "   #     std_ydata=np.std(ydata)\n",
        "   #     std_ydata=round(std_ydata,2)\n",
        "   #     #print(\"Resp Err NO FILTER =\",std_ydataNOFILTER,\"Response Error After Outlier Removal =\",std_ydata,\", cutoff threshold:\", int(ErrorCutoff), \",\", \"trials removed\", int(ydataAfterConditions - len(ydata)))\n",
        "   # else:\n",
        "   #     std_ydata=np.std(ydata)\n",
        "   #     std_ydata=round(std_ydata,2)\n",
        "   #     #print(\"Response Error No Filter =\",std_ydata)\n",
        "   # \n",
        "    \n",
        "    #compensates for biases\n",
        "    #ydata = ydata-np.mean(ydata)---> DA FARE??  \n",
        "    \n",
        "    \n",
        "    #DEFINING GRAPH VARIABLES\n",
        "    ydata_neg = ydata #error, with negative values   #rel_error e #abs_error\n",
        "    ydata = abs(ydata) #absolute error\n",
        "\n",
        "        \n",
        "    #### CREATE A DATAFRAME\n",
        "    df = pd.DataFrame({\"ActualZ\": xdata, \"Error\": ydata, \"Error_neg\": ydata_neg})\n",
        "    pd.set_option(\"max_rows\", None)\n",
        "    df = df.sort_values(by=[\"ActualZ\"])\n",
        "    #print(df)\n",
        "\n",
        "    #Bin the errors in 49 bins (3 current orientations in each bin)\n",
        "    #slices = np.linspace(1, 145, (49), endpoint=False).astype(int)\n",
        "    #print(slices)\n",
        "    #print(len(slices))\n",
        "\n",
        "    slices2 = []\n",
        "    for x in range(-2,145,3):\n",
        "        x2 = x+3\n",
        "        #print(x2)\n",
        "        slices2.append(x2)\n",
        "    #print(slices2)\n",
        "    #print(len(slices2))\n",
        "    \n",
        "    TotBins= []\n",
        "    for x in slices2:\n",
        "        #print(x)\n",
        "        bins_= df[(df[\"ActualZ\"]>=x) & (df[\"ActualZ\"]<x+3)]\n",
        "        #print(bins_)\n",
        "        bins_2 = list(bins_[\"Error_neg\"])\n",
        "        #print(bins_2)\n",
        "        TotBins.append(bins_2)                  \n",
        "    \n",
        "    \n",
        "    TotBins_allsubjects.append(TotBins)\n",
        "        \n",
        "    print(TotBins_allsubjects)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#############################################Runs Function#############################################\n",
        "#pathSTART = \"/Users/\" + FolderPathUsers + \"/SuperRecognizers/TryConsistencyAnalysis/Data/\"\n",
        "\n",
        "\n",
        "All_ErrorOut = []\n",
        "TotAge = []\n",
        "TotAmplitude = []\n",
        "TotAmplitude2 = []\n",
        "Totmean = []\n",
        "Totmean_abs = []\n",
        "SEM = []\n",
        "Totmean1 = []\n",
        "\n",
        "TotBins_allsubjects = []\n",
        "\n",
        "\n",
        "\n",
        "print(pathSTART)\n",
        "Subj = sorted(os.listdir(pathSTART))\n",
        "print(Subj)\n",
        "TotalSubj = np.arange(0, len(Subj))\n",
        "print(TotalSubj)\n",
        "for i in TotalSubj:\n",
        "    path = pathSTART + Subj[i] + \"/\"\n",
        "    print(\"SUBJECT FROM:\")\n",
        "    print(path)\n",
        "    RunAnalysis(path)\n",
        "    print()\n",
        "    print()\n",
        "    print()\n",
        "\n"
      ],
      "metadata": {
        "id": "tk1kiUDV1V0u",
        "outputId": "858f0b3e-44ab-4bbb-fd01-17e8be070092",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c65123fe7fff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathSTART\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mSubj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathSTART\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSubj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pathSTART' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sr970eHeFXfQ"
      },
      "outputs": [],
      "source": [
        "#print(TotBins_allsubjects[9])\n",
        "\n",
        "a= np.arange(1,147,3)\n",
        "print(a, len(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M600dFEAFXfQ"
      },
      "outputs": [],
      "source": [
        "### Within-subject consistency--> Bootstrap method to estimate split-half correlations \n",
        "from lmfit.models import GaussianModel\n",
        "from scipy.stats import linregress\n",
        "import random\n",
        "import scipy.stats as stats\n",
        "\n",
        "\n",
        "def split_list(a_list):\n",
        "    half = len(a_list)//2\n",
        "    return a_list[:half], a_list[half:]\n",
        "\n",
        "    \n",
        "def flatten(seq): \n",
        "    l = []\n",
        "    for elt in seq:\n",
        "        t = type(elt)\n",
        "        if t is tuple or t is list:\n",
        "            for elt2 in flatten(elt):\n",
        "                l.append(elt2)\n",
        "        else:\n",
        "            l.append(elt)\n",
        "    return l\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#On each iteration, for each observer and each binned morph, randomly split the responses into two halves \n",
        "\n",
        "All_iterations_Average_r_Pearson = []\n",
        "iterations = 0\n",
        "\n",
        "\n",
        "#CorrSubj = []\n",
        "#for b in range (1000):\n",
        "#    bin_n1 = []\n",
        "#    bin_n2 = []\n",
        "#    Tot_MEANbin_1 = []\n",
        "#    Tot_MEANbin_2 = []\n",
        "#    NAN_ = 0\n",
        "#    iterations += 1\n",
        "        \n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "ALL_CorrSubj = []\n",
        "for x in range(len(Subj)):\n",
        "    CorrSubj = []\n",
        "    for b in range (1000):\n",
        "        mean_bin1_singleSubject = []\n",
        "        mean_bin2_singleSubject = []\n",
        "        for bin in range(49):\n",
        "            #print(bin)\n",
        "            #randomly shuffle the values in the bin\n",
        "            actual_bin = (TotBins_allsubjects[x])[bin]\n",
        "            random.shuffle(actual_bin)\n",
        "            \n",
        "            #randomly split the responses into two halves \n",
        "            a, b = split_list(actual_bin)\n",
        "            \n",
        "            if len(a) > len(b):\n",
        "                del a[-1]\n",
        "            if len(b) > len(a):\n",
        "                del b[-1]\n",
        "                            \n",
        "            \n",
        "           #alert if a bin is = []        \n",
        "            if a == [] and b == []:\n",
        "                a = 0\n",
        "                b = 0\n",
        "                #print(\"NANN\")\n",
        "                #print(bin)\n",
        "           #if a bin = [], repeat the value of the other bin (this is to avoid NaN)\n",
        "            if a == []:\n",
        "                a = b\n",
        "                #NAN_ = NAN_ + 1\n",
        "                #print(\"NAN\")\n",
        "                \n",
        "            if b == []:\n",
        "                b = a\n",
        "                #NAN_ = NAN_ + 1\n",
        "                #print(\"NAN\")\n",
        "    \n",
        "            #calculate the mean response errors for each half\n",
        "            mean_bin1 = mean(a)\n",
        "            mean_bin2 = mean(b)\n",
        "            \n",
        "            mean_bin1_singleSubject.append(mean_bin1)\n",
        "            mean_bin2_singleSubject.append(mean_bin2)\n",
        "            \n",
        "        #The two halves are correlated--> Pearson’s r value  \n",
        "        corr, _ = pearsonr(mean_bin1_singleSubject, mean_bin2_singleSubject)\n",
        "        #print(corr)\n",
        "        corr = np.arctanh(corr)\n",
        "        #print(corr)\n",
        "        CorrSubj.append(corr)\n",
        "\n",
        "    print(mean(CorrSubj))\n",
        "    ALL_CorrSubj.append(mean(CorrSubj))\n",
        "    \n",
        "print(ALL_CorrSubj)\n",
        "print(mean(ALL_CorrSubj))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LULMcd3FXfR"
      },
      "outputs": [],
      "source": [
        "print(All_iterations_Average_r_Pearson)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrMu3u_UFXfR"
      },
      "outputs": [],
      "source": [
        "#PLOT BAR GRAPH WITHIN-SUBJECT CONSISTENCY\n",
        "\n",
        "list1 = [1]\n",
        "\n",
        "fig1 = plt.figure(figsize=(10,8))\n",
        "\n",
        "plt.bar(list1, meanAmp, width=0.8, align='center')\n",
        "\n",
        "plt.axis([0, 2, -0.1, 0.8]) \n",
        "#plt.xticks([\"Control group\"])\n",
        "#plt.yticks(np.arange(0, 60, 5))\n",
        "\n",
        "plt.errorbar(list1, meanAmp, yerr=(conf_interval1-meanAmp, meanAmp-conf_interval2), fmt=' ', ecolor='black', capsize=5)\n",
        "\n",
        "title = \"WITHIN-SUBJECT CONSISTENCY\\n\" + \"Mean: \" + str(np.round(meanAmp, decimals = 2))  \n",
        "plt.title(title, fontsize = 20)\n",
        "plt.xlabel(\"\\nRadiologist group\", fontsize = 20)\n",
        "plt.ylabel(\"Pearson's R\", fontsize = 20)\n",
        "plt.tick_params(labelsize = 15 ,bottom=False, labelbottom=False)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "FigName = \"/Users/\" + FolderPathUsers + \"/SuperRecognizers/TryConsistencyAnalysis/all_graphs/Bootstrap_Within-subjects_BarGraph_3bin.png\"\n",
        "\n",
        "\n",
        "fig1.tight_layout()\n",
        "fig1.savefig(FigName, dpi=600)\n",
        "\n",
        "plt.clf()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBpOXd0yFXfR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Copy of Copy of Face_Id_Analysis_SD_Ramon_Fit_RM_Split-half analysis_3BINS__.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}